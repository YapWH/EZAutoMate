{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "# Scientific computing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Time series analysis libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.models import models\n",
    "\n",
    "# Utility libraries\n",
    "import optuna # Hyperparameter optimization\n",
    "import tqdm  # Progress bar visualization\n",
    "\n",
    "# Custom libraries\n",
    "from effKAN import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read car data\n",
    "data = pd.read_csv('./data/car_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = data.sample(frac=0.01, random_state=42)\n",
    "\n",
    "object_columns = sampled_data.select_dtypes(include=['object']).columns\n",
    "object_columns = object_columns.drop(['Car_id','Customer Name','Date'])\n",
    "\n",
    "if not os.path.exists('./graph/univariate'):\n",
    "    os.makedirs('./graph/univariate')\n",
    "\n",
    "# For Date Count\n",
    "sales_counts = sampled_data['Date'].value_counts().sort_index().reset_index()\n",
    "sales_counts.columns = ['Date', 'Sales']\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.barplot(x='Date', y='Sales', data=sales_counts, hue='Date')\n",
    "plt.title('Sales Count by Date')\n",
    "plt.xticks([])\n",
    "plt.savefig('./graph/univariate/sales_count_by_date.png')\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(object_columns)):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    sns.countplot(x=object_columns[i], data=sampled_data, hue=object_columns[i])\n",
    "    plt.title(f'Count Plot for {object_columns[i]}')\n",
    "    plt.xlabel(object_columns[i])\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(f'./graph/univariate/{object_columns[i]}_countplot.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.kdeplot(x=sampled_data['Annual Income'], data=sampled_data)\n",
    "plt.title(f'KDE Plot for Annual Income')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('KDE')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig(f'./graph/univariate/Annual_Income_KDEplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./graph/bivariate'):\n",
    "    os.makedirs('./graph/bivariate')\n",
    "\n",
    "for i in range(len(object_columns)):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    sns.violinplot(x=object_columns[i], y='Price ($)', data=sampled_data, hue=object_columns[i])\n",
    "    plt.title(f'Violin Plot for {object_columns[i]} vs Price')\n",
    "    plt.xlabel(object_columns[i])\n",
    "    plt.ylabel('Price')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(f'./graph/bivariate/{object_columns[i]}_vs_price_violinplot.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.boxplot(x=sampled_data['Annual Income'], data=sampled_data)\n",
    "plt.title(f'KDE Plot for Annual Income')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('KDE')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig(f'./graph/bivariate/Annual_Income_KDE_plot.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./graph/multivariate'):\n",
    "    os.makedirs('./graph/multivariate')\n",
    "\n",
    "sns.pairplot(sampled_data)\n",
    "plt.savefig('./graph/multivariate/pairplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert, Encode, Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert `Date` to independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Day'] = data['Date'].dt.day\n",
    "\n",
    "data = data.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Car_id','Customer Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "le = LabelEncoder()\n",
    "for i in range(len(object_columns)):\n",
    "    data[object_columns[i]] = le.fit_transform(data[object_columns[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = data.select_dtypes(include=['int64', 'float64', 'int32']).columns\n",
    "num_columns = num_columns.drop(['Price ($)'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "for i in range(len(num_columns)):\n",
    "    data[num_columns[i]] = scaler.fit_transform(data[num_columns[i]].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = data.describe()\n",
    "\n",
    "print(\"Summary of Statistics:\")\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness and kurtosis\n",
    "skewness = data.skew()\n",
    "kurtosis = data.kurtosis()\n",
    "# Display skewness and kurtosis values\n",
    "print(\"\\nSkewness:\")\n",
    "print(skewness)\n",
    "print(\"\\nKurtosis:\")\n",
    "print(kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", linewidths=0.5, fmt = \".3f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Multicollinearity\n",
    "y = data.drop([\"Price ($)\"], axis =1)\n",
    "X = sm.add_constant(y)\n",
    "\n",
    "# Calculate VIF for each variable\n",
    "vif = pd.DataFrame()\n",
    "vif[\"variable\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_vif_variables = vif[vif[\"VIF\"] >= 5][\"variable\"]\n",
    "regression_data = X.drop(high_vif_variables, axis=1)\n",
    "\n",
    "regression_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = regression_data\n",
    "y = data['Price ($)']\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_X = torch.tensor(train_X.values, dtype=torch.float32).to(device)\n",
    "train_y = torch.tensor(train_y.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "test_X = torch.tensor(test_X.values, dtype=torch.float32).to(device)\n",
    "test_y = torch.tensor(test_y.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "trainset = TensorDataset(train_X, train_y)\n",
    "trainloader = DataLoader(trainset, batch_size=512, shuffle=True)\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    hidden_layers = trial.suggest_int(\"n_hidden_layers\", 2, 5)\n",
    "    hidden_units = trial.suggest_int(\"hidden_units\", 2, 64)\n",
    "\n",
    "    model = KAN([12] + [hidden_units] * hidden_layers + [1])\n",
    "    model.to(torch.float32).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, foreach=False)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    with tqdm(trainloader, desc=\"Training\") as pbar:\n",
    "        for _, (X, y) in enumerate(pbar):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"Best Learning Rate:\", best_trial.params[\"learning_rate\"])\n",
    "print(\"Best Weight Decay:\", best_trial.params[\"weight_decay\"])\n",
    "print(\"Best Number of Hidden Layers:\", best_trial.params[\"n_hidden_layers\"])\n",
    "print(\"Best Hidden Units:\", best_trial.params[\"hidden_units\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_learning_rate = best_trial.params[\"learning_rate\"]\n",
    "best_weight_decay = best_trial.params[\"weight_decay\"]\n",
    "best_hidden_layers = best_trial.params[\"n_hidden_layers\"]\n",
    "best_hidden_units = best_trial.params[\"hidden_units\"]\n",
    "\n",
    "model = KAN([12] + [best_hidden_units] * best_hidden_layers + [1])\n",
    "model.to(torch.float32).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay, foreach=False)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
    "        for i, (X, y) in enumerate(pbar):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "    train_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Training Loss\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X).cpu().numpy()\n",
    "\n",
    "mse = mean_squared_error(test_y.cpu(), y_pred)\n",
    "r2 = r2_score(test_y.cpu(), y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse/len(test_y)}\")\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.resnet = models.resnet18()\n",
    "\n",
    "        self.resnet.conv1 = nn.Conv2d(self.num_classes, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(512, 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_classes, 1, 1)\n",
    "        x = self.resnet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    model = ResNet(12)\n",
    "    model.to(torch.float32).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, foreach=False)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    with tqdm(trainloader, desc=\"Training\") as pbar:\n",
    "        for _, (X, y) in enumerate(pbar):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"Best Learning Rate:\", best_trial.params[\"learning_rate\"])\n",
    "print(\"Best Weight Decay:\", best_trial.params[\"weight_decay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_learning_rate = best_trial.params[\"learning_rate\"]\n",
    "best_weight_decay = best_trial.params[\"weight_decay\"]\n",
    "\n",
    "model = ResNet(12)\n",
    "model.to(device)\n",
    "model.float()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay, foreach=False)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
    "        for i, (X, y) in enumerate(pbar):\n",
    "            X, y  = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "    train_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Training Loss\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X).cpu().numpy()\n",
    "\n",
    "mse = mean_squared_error(test_y.cpu(), y_pred)\n",
    "r2 = r2_score(test_y.cpu(), y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse/len(test_y)}\")\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Sequence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/car_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.loc[:, ['Date', 'Price ($)']]\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.set_index('Date', inplace = True)\n",
    "data_monthly_mean = new_df.resample('M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving Average \n",
    "# Calculate Simple Moving Average (SMA)\n",
    "sma_period = 10\n",
    "new_df['SMA'] = data_monthly_mean['Price ($)'].rolling(window=sma_period).mean().reindex(new_df.index, method='ffill')\n",
    "\n",
    "# Calculate Exponential Moving Average (EMA)\n",
    "ema_period = 10\n",
    "data_monthly_mean['EMA'] = data_monthly_mean['Price ($)'].ewm(span=ema_period, adjust=False).mean()\n",
    "new_df['EMA'] = data_monthly_mean['EMA'].reindex(new_df.index, method='ffill')\n",
    "\n",
    "# Calculate Cummulative Moving Average (CMA)\n",
    "new_df['CMA'] = data_monthly_mean['Price ($)'].expanding(min_periods=1).mean().reindex(new_df.index, method='ffill')\n",
    "\n",
    "# Calculate Weighted Moving Average (WMA)\n",
    "wma_period = 10 \n",
    "weights = pd.Series(range(1, wma_period + 1))\n",
    "def weighted_moving_average(prices):\n",
    "    return np.dot(prices, weights) / weights.sum()\n",
    "\n",
    "new_df['WMA'] = data_monthly_mean['Price ($)'].rolling(window=wma_period).apply(weighted_moving_average, raw=True).reindex(new_df.index, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(data_monthly_mean['Price ($)'], label = 'Price')\n",
    "plt.plot(new_df['SMA'], label = 'SMA')\n",
    "plt.plot(new_df['EMA'], label = 'EMA')\n",
    "plt.plot(new_df['CMA'], label = 'CMA')\n",
    "plt.plot(new_df['WMA'], label = 'WMA')\n",
    "plt.title('Moving Averages for Monthly Mean Total Price ($)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series):\n",
    "    result = adfuller(series, autolag = 'AIC')\n",
    "    print(f'ADF Statistics: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    print(f'Critical values: {result[4]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Data ADF Test:\")\n",
    "adf_test(data_monthly_mean['Price ($)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 8))\n",
    "\n",
    "# ACF plot\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_acf(data_monthly_mean['Price ($)'], lags = 10, ax = plt.gca())\n",
    "plt.title('Autocorrelated Function (ACF)')\n",
    "\n",
    "# PACF plot\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_pacf(data_monthly_mean['Price ($)'], lags = 10, ax = plt.gca())\n",
    "plt.title('Partial Autocorrelated Function (PACF)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "acf_values = acf(data_monthly_mean['Price ($)'], nlags = 10)\n",
    "pacf_values = pacf(data_monthly_mean['Price ($)'], nlags = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval threshold\n",
    "n = len(data_monthly_mean['Price ($)'])\n",
    "threshold = 1.96/np.sqrt(n)\n",
    "\n",
    "# Count significant values for p and q\n",
    "significant_p_values = sum(abs(pacf_values[1:]) > threshold)\n",
    "significant_q_values = sum(abs(acf_values[1:]) > threshold)\n",
    "\n",
    "print(f\"Number significant p values: {significant_p_values}\")\n",
    "print(f\"Number significant q values: {significant_q_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 2\n",
    "d = 0 \n",
    "q = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(data_monthly_mean['Price ($)'], order = (p, d, q))\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
