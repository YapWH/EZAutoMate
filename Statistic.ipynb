{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "# Scientific computing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Time series analysis libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import models\n",
    "\n",
    "# Utility libraries\n",
    "import optuna # Hyperparameter optimization\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_slice, plot_param_importances\n",
    "from tqdm import tqdm  # Progress bar visualization\n",
    "\n",
    "# Custom libraries\n",
    "from effKAN import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read car data\n",
    "data = pd.read_csv('./data/car_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = data.sample(frac=0.01, random_state=42)\n",
    "\n",
    "object_columns = sampled_data.select_dtypes(include=['object']).columns\n",
    "object_columns = object_columns.drop(['Car_id','Customer Name','Date'])\n",
    "\n",
    "if not os.path.exists('./graph/univariate'):\n",
    "    os.makedirs('./graph/univariate')\n",
    "\n",
    "# For Date Count\n",
    "sales_counts = sampled_data['Date'].value_counts().sort_index().reset_index()\n",
    "sales_counts.columns = ['Date', 'Sales']\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.barplot(x='Date', y='Sales', data=sales_counts, hue='Date')\n",
    "plt.title('Sales Count by Date')\n",
    "plt.xticks([])\n",
    "plt.savefig('./graph/univariate/sales_count_by_date.png')\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(object_columns)):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    sns.countplot(x=object_columns[i], data=sampled_data, hue=object_columns[i])\n",
    "    plt.title(f'Count Plot for {object_columns[i]}')\n",
    "    plt.xlabel(object_columns[i])\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(f'./graph/univariate/{object_columns[i]}_countplot.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.kdeplot(x=sampled_data['Annual Income'], data=sampled_data)\n",
    "plt.title(f'KDE Plot for Annual Income')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('KDE')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig(f'./graph/univariate/Annual_Income_KDEplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./graph/bivariate'):\n",
    "    os.makedirs('./graph/bivariate')\n",
    "\n",
    "for i in range(len(object_columns)):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    sns.violinplot(x=object_columns[i], y='Price ($)', data=sampled_data, hue=object_columns[i])\n",
    "    plt.title(f'Violin Plot for {object_columns[i]} vs Price')\n",
    "    plt.xlabel(object_columns[i])\n",
    "    plt.ylabel('Price')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(f'./graph/bivariate/{object_columns[i]}_vs_price_violinplot.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.boxplot(x=sampled_data['Annual Income'], data=sampled_data)\n",
    "plt.title(f'KDE Plot for Annual Income')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('KDE')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig(f'./graph/bivariate/Annual_Income_KDE_plot.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./graph/multivariate'):\n",
    "    os.makedirs('./graph/multivariate')\n",
    "\n",
    "sns.pairplot(sampled_data)\n",
    "plt.savefig('./graph/multivariate/pairplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert, Encode, Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert `Date` to independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Day'] = data['Date'].dt.day\n",
    "\n",
    "data = data.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Car_id','Customer Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "le = LabelEncoder()\n",
    "for i in range(len(object_columns)):\n",
    "    data[object_columns[i]] = le.fit_transform(data[object_columns[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = data.select_dtypes(include=['int64', 'float64', 'int32']).columns\n",
    "num_columns = num_columns.drop(['Price ($)'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "for i in range(len(num_columns)):\n",
    "    data[num_columns[i]] = scaler.fit_transform(data[num_columns[i]].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = data.describe()\n",
    "\n",
    "print(\"Summary of Statistics:\")\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness and kurtosis\n",
    "skewness = data.skew()\n",
    "kurtosis = data.kurtosis()\n",
    "# Display skewness and kurtosis values\n",
    "print(\"\\nSkewness:\")\n",
    "print(skewness)\n",
    "print(\"\\nKurtosis:\")\n",
    "print(kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", linewidths=0.5, fmt = \".3f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Multicollinearity\n",
    "y = data.drop([\"Price ($)\"], axis =1)\n",
    "X = sm.add_constant(y)\n",
    "\n",
    "# Calculate VIF for each variable\n",
    "vif = pd.DataFrame()\n",
    "vif[\"variable\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_vif_variables = vif[vif[\"VIF\"] >= 5][\"variable\"]\n",
    "regression_data = X.drop(high_vif_variables, axis=1)\n",
    "\n",
    "regression_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = regression_data\n",
    "y = data['Price ($)']\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_X = torch.tensor(train_X.values, dtype=torch.float32).to(device)\n",
    "train_y = torch.tensor(train_y.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "test_X = torch.tensor(test_X.values, dtype=torch.float32).to(device)\n",
    "test_y = torch.tensor(test_y.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "trainset = TensorDataset(train_X, train_y)\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    hidden_layers = trial.suggest_int(\"n_hidden_layers\", 2, 5)\n",
    "    hidden_units = trial.suggest_int(\"hidden_units\", 2, 64)\n",
    "    batchsize = trial.suggest_int(\"batchsize\", 32, 2048)\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "    model = KAN([train_X.shape[1], hidden_units * hidden_layers, 1])\n",
    "    model.to(torch.float32).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, foreach=False)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for _, (X, y) in enumerate(trainloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"Best Learning Rate:\", best_trial.params[\"learning_rate\"])\n",
    "print(\"Best Weight Decay:\", best_trial.params[\"weight_decay\"])\n",
    "print(\"Best Number of Hidden Layers:\", best_trial.params[\"n_hidden_layers\"])\n",
    "print(\"Best Hidden Units:\", best_trial.params[\"hidden_units\"])\n",
    "print(\"Best Batch Size:\", best_trial.params[\"batchsize\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_learning_rate = best_trial.params[\"learning_rate\"]\n",
    "best_weight_decay = best_trial.params[\"weight_decay\"]\n",
    "best_hidden_layers = best_trial.params[\"n_hidden_layers\"]\n",
    "best_hidden_units = best_trial.params[\"hidden_units\"]\n",
    "best_batchsize = best_trial.params[\"batchsize\"]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=best_batchsize, shuffle=True)\n",
    "\n",
    "model = KAN([train_X.shape[1], best_hidden_units * best_hidden_layers, 1])\n",
    "model.to(torch.float32).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay, foreach=False)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
    "        for i, (X, y) in enumerate(pbar):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "    train_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Training Loss\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X).cpu().numpy()\n",
    "\n",
    "mse = mean_squared_error(test_y.cpu(), y_pred)\n",
    "r2 = r2_score(test_y.cpu(), y_pred)\n",
    "mae = mean_absolute_error(test_y.cpu(), y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.resnet = models.resnet18()\n",
    "\n",
    "        self.resnet.conv1 = nn.Conv2d(self.num_classes, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(512, 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_classes, 1, 1)\n",
    "        x = self.resnet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    batchsize = trial.suggest_int(\"batchsize\", 32, 2048)\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "    model = ResNet(train_X.shape[1])\n",
    "    model.to(torch.float32).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, foreach=False)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    with tqdm(trainloader, desc=\"Training\") as pbar:\n",
    "        for _, (X, y) in enumerate(pbar):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"Best Learning Rate:\", best_trial.params[\"learning_rate\"])\n",
    "print(\"Best Weight Decay:\", best_trial.params[\"weight_decay\"])\n",
    "print(\"Best Batch Size:\", best_trial.params[\"batchsize\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_learning_rate = best_trial.params[\"learning_rate\"]\n",
    "best_weight_decay = best_trial.params[\"weight_decay\"]\n",
    "best_batchsize = best_trial.params[\"batchsize\"]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=best_batchsize, shuffle=True)\n",
    "\n",
    "model = ResNet(train_X.shape[1])\n",
    "model.to(device)\n",
    "model.float()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay, foreach=False)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
    "        for i, (X, y) in enumerate(pbar):\n",
    "            X, y  = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "    train_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Training Loss\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X).cpu().numpy()\n",
    "\n",
    "mse = mean_squared_error(test_y.cpu(), y_pred)\n",
    "r2 = r2_score(test_y.cpu(), y_pred)\n",
    "mae = mean_absolute_error(test_y.cpu(), y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Sequence Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/car_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.loc[:, ['Date', 'Price ($)']]\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.fillna(new_df.mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.set_index('Date', inplace = True)\n",
    "data_monthly_mean = new_df.resample('M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving Average \n",
    "# Calculate Simple Moving Average (SMA)\n",
    "sma_period = 10\n",
    "new_df['SMA'] = data_monthly_mean['Price ($)'].rolling(window=sma_period).mean().reindex(new_df.index, method='ffill')\n",
    "\n",
    "# Calculate Exponential Moving Average (EMA)\n",
    "ema_period = 10\n",
    "data_monthly_mean['EMA'] = data_monthly_mean['Price ($)'].ewm(span=ema_period, adjust=False).mean()\n",
    "new_df['EMA'] = data_monthly_mean['EMA'].reindex(new_df.index, method='ffill')\n",
    "\n",
    "# Calculate Cummulative Moving Average (CMA)\n",
    "new_df['CMA'] = data_monthly_mean['Price ($)'].expanding(min_periods=1).mean().reindex(new_df.index, method='ffill')\n",
    "\n",
    "# Calculate Weighted Moving Average (WMA)\n",
    "wma_period = 10 \n",
    "weights = pd.Series(range(1, wma_period + 1))\n",
    "def weighted_moving_average(prices):\n",
    "    return np.dot(prices, weights) / weights.sum()\n",
    "\n",
    "new_df['WMA'] = data_monthly_mean['Price ($)'].rolling(window=wma_period).apply(weighted_moving_average, raw=True).reindex(new_df.index, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(data_monthly_mean['Price ($)'], label = 'Price')\n",
    "plt.plot(new_df['SMA'], label = 'SMA')\n",
    "plt.plot(new_df['EMA'], label = 'EMA')\n",
    "plt.plot(new_df['CMA'], label = 'CMA')\n",
    "plt.plot(new_df['WMA'], label = 'WMA')\n",
    "plt.title('Moving Averages for Monthly Mean Total Price ($)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series):\n",
    "    adf_statistic, p_value, _, _, critical_values, _= adfuller(series)\n",
    "    print(f'ADF Statistics: {adf_statistic:.4f}')\n",
    "    print(f'p-value: {p_value:.4f}')\n",
    "    print('Critical values:')\n",
    "    for key, value in critical_values.items():\n",
    "        print(f'   {key}: {value:.4f}')\n",
    "    \n",
    "print(\"Original Data ADF Test:\")\n",
    "adf_test(data_monthly_mean['Price ($)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 8))\n",
    "\n",
    "# ACF plot\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_acf(data_monthly_mean['Price ($)'], lags = 10, ax = plt.gca())\n",
    "plt.title('Autocorrelated Function (ACF)')\n",
    "\n",
    "# PACF plot\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_pacf(data_monthly_mean['Price ($)'], lags = 10, ax = plt.gca())\n",
    "plt.title('Partial Autocorrelated Function (PACF)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "acf_values = acf(data_monthly_mean['Price ($)'], nlags = 10)\n",
    "pacf_values = pacf(data_monthly_mean['Price ($)'], nlags = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval threshold\n",
    "n = len(data_monthly_mean['Price ($)'])\n",
    "threshold = 1.96/np.sqrt(n)\n",
    "\n",
    "# Count significant values for p and q\n",
    "significant_p_values = sum(abs(pacf_values[1:]) > threshold)\n",
    "significant_q_values = sum(abs(acf_values[1:]) > threshold)\n",
    "\n",
    "print(f\"Number significant p values: {significant_p_values}\")\n",
    "print(f\"Number significant q values: {significant_q_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 2\n",
    "d = 0 \n",
    "q = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(data_monthly_mean['Price ($)'], order = (p, d, q))\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = results.resid\n",
    "plt.figure(figsize = (10, 4))\n",
    "plt.plot(residuals)\n",
    "plt.title('Residuals of ARIMA model')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data_monthly_mean)*0.8)\n",
    "train, validation = data_monthly_mean.iloc[:train_size], data_monthly_mean.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = results.predict(start = validation.index[0], end = validation.index[-1], type = 'levels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_score = rmse(validation['Price ($)'], predictions)\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 4))\n",
    "plt.plot(validation['Price ($)'], label = 'Actual')\n",
    "plt.plot(predictions, color = 'red', label = 'ARIMA Predicted')\n",
    "plt.title('ARIMA Model Validation - Actual vs. Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_steps = 12\n",
    "forecast = results.get_forecast(steps = forecast_steps)\n",
    "predicted_values = forecast.predicted_mean\n",
    "print(predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 6))\n",
    "plt.plot(data_monthly_mean['Price ($)'], label = 'Original')\n",
    "plt.plot(predicted_values.index, predicted_values.values, color = 'red', label = 'Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('ARIMA Predictions from Price Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.values.astype(float).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from xlstm import xLSTM\n",
    "\n",
    "# Define the dataset class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index + self.seq_len >= len(self.data):\n",
    "            raise IndexError(\"Index out of range for dataset\")\n",
    "        x = self.data[index:index + self.seq_len]\n",
    "        y = self.data[index + self.seq_len]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# # Load the data\n",
    "# def load_data(file_path):\n",
    "#     df = pd.read_csv(file_path, usecols=[1])\n",
    "#     data = df.values.astype(float).reshape(-1)\n",
    "#     return data\n",
    "\n",
    "# Define the evaluation function for validation\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            output, _ = model(x_batch.unsqueeze(-1))\n",
    "            loss = criterion(output[-1], y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Define the evaluation function for full sequence\n",
    "def evaluate_model_full_sequence(model, data, device, seq_len):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    data = torch.tensor(data, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(seq_len, len(data)):\n",
    "            input_seq = data[i-seq_len:i].unsqueeze(1)  # Shape: (seq_len, 1, 1)\n",
    "            output, _ = model(input_seq)\n",
    "            prediction = output[-1].item()\n",
    "            predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, device, num_epochs=20):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for x_batch, y_batch in train_dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(x_batch.unsqueeze(-1))\n",
    "            loss = criterion(output[-1], y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        train_losses.append(total_train_loss / len(train_dataloader))\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss = evaluate_model(model, val_dataloader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}\")\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Parameters\n",
    "# file_path = 'AirPassengers.csv'  # Change this to your dataset path\n",
    "seq_len = 16\n",
    "batch_size = 512\n",
    "hidden_size = 128\n",
    "num_heads = 1\n",
    "layers = ['m', 's', 'm']\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "# data = load_data(file_path)\n",
    "dataset = TimeSeriesDataset(data, seq_len)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_size = int(len(dataset) * 0.6)\n",
    "val_size = int(len(dataset) * 0.2)\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = xLSTM(input_size=1, hidden_size=hidden_size, num_heads=num_heads, layers=layers).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "# Train model\n",
    "train_losses, val_losses = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, device, num_epochs)\n",
    "\n",
    "# Evaluate model on the test set\n",
    "test_data = data[-(test_size + seq_len):]\n",
    "predictions = evaluate_model_full_sequence(model, test_data, device, seq_len)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(data, label='Actual')\n",
    "plt.plot(range(len(data) - test_size, len(data)), predictions, label='Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
