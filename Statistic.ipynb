{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Import, read, head nad dtype\n",
    "2. Check for NA values\n",
    "3. Graphs\n",
    "    - Univariate Analysis\n",
    "    - Bivariate Analysis\n",
    "    - Multivariate Analysis\n",
    "4. Convert, Encode and Normalization\n",
    "5. Factor Analysis\n",
    "    1. Check Data Adequacy[KMO, Bartllet]\n",
    "    2. *Feature Extraction[Scree Plot, Kaiser criterion, PCA, Maximum Likelihood]\n",
    "    3. Factor Rotation[Promax, Varimax]\n",
    "6. Model Training and Prediction[KAN]\n",
    "7. Model Performance Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy as sp\n",
    "import random\n",
    "from factor_analyzer import FactorAnalyzer,calculate_bartlett_sphericity,calculate_kmo\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read car data\n",
    "data = pd.read_csv('./data/car_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = data.sample(frac=0.01, random_state=42)\n",
    "\n",
    "object_columns = sampled_data.select_dtypes(include=['object']).columns\n",
    "object_columns = object_columns.drop(['Car_id','Customer Name','Date'])\n",
    "\n",
    "if not os.path.exists('./graph/univariate'):\n",
    "    os.makedirs('./graph/univariate')\n",
    "\n",
    "# For Date Count\n",
    "sales_counts = sampled_data['Date'].value_counts().sort_index().reset_index()\n",
    "sales_counts.columns = ['Date', 'Sales']\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.barplot(x='Date', y='Sales', data=sales_counts, hue='Date')\n",
    "plt.title('Sales Count by Date')\n",
    "plt.xticks([])\n",
    "plt.savefig('./graph/univariate/sales_count_by_date.png')\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(object_columns)):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    sns.countplot(x=object_columns[i], data=sampled_data, hue=object_columns[i])\n",
    "    plt.title(f'Count Plot for {object_columns[i]}')\n",
    "    plt.xlabel(object_columns[i])\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(f'./graph/univariate/{object_columns[i]}_countplot.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.kdeplot(x=sampled_data['Annual Income'], data=sampled_data)\n",
    "plt.title(f'KDE Plot for Annual Income')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('KDE')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig(f'./graph/univariate/Annual_Income_KDEplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./graph/bivariate'):\n",
    "    os.makedirs('./graph/bivariate')\n",
    "\n",
    "for i in range(len(object_columns)):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    sns.violinplot(x=object_columns[i], y='Price ($)', data=sampled_data, hue=object_columns[i])\n",
    "    plt.title(f'Violin Plot for {object_columns[i]} vs Price')\n",
    "    plt.xlabel(object_columns[i])\n",
    "    plt.ylabel('Price')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(f'./graph/bivariate/{object_columns[i]}_vs_price_violinplot.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.boxplot(x=sampled_data['Annual Income'], data=sampled_data)\n",
    "plt.title(f'KDE Plot for Annual Income')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('KDE')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig(f'./graph/bivariate/Annual_Income_KDE_plot.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./graph/multivariate'):\n",
    "    os.makedirs('./graph/multivariate')\n",
    "\n",
    "sns.pairplot(sampled_data)\n",
    "plt.savefig('./graph/multivariate/pairplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert, Encode, Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert `Date` to independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Day'] = data['Date'].dt.day\n",
    "\n",
    "data = data.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "#Label Encoding\n",
    "le = LabelEncoder()\n",
    "for i in range(len(object_columns)):\n",
    "    data[object_columns[i]] = le.fit_transform(data[object_columns[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = data.select_dtypes(include=['int64', 'float64', 'int32']).columns\n",
    "\n",
    "#Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "for i in range(len(num_columns)):\n",
    "    data[num_columns[i]] = scaler.fit_transform(data[num_columns[i]].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = data.describe()\n",
    "\n",
    "print(\"Summary of Statistics:\")\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness and kurtosis\n",
    "skewness = data.skew()\n",
    "kurtosis = data.kurtosis()\n",
    "# Display skewness and kurtosis values\n",
    "print(\"\\nSkewness:\")\n",
    "print(skewness)\n",
    "print(\"\\nKurtosis:\")\n",
    "print(kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", linewidths=0.5, fmt = \".3f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Multicollinearity\n",
    "y = data.drop([\"Price ($)\"], axis =1)\n",
    "X = sm.add_constant(y)\n",
    "\n",
    "# Calculate VIF for each variable\n",
    "vif = pd.DataFrame()\n",
    "vif[\"variable\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_vif_variables = vif[vif[\"VIF\"] >= 5][\"variable\"]\n",
    "regression_data = X.drop(high_vif_variables, axis=1)\n",
    "\n",
    "regression_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from effKAN import KAN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = regression_data.drop(['const'], axis=1)\n",
    "y = data['Price ($)']\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_X = torch.tensor(train_X.values, dtype=torch.float32).to(device)\n",
    "train_y = torch.tensor(train_y.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "test_X = torch.tensor(test_X.values, dtype=torch.float32).to(device)\n",
    "test_y = torch.tensor(test_y.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "trainset = TensorDataset(train_X, train_y)\n",
    "trainloader = DataLoader(trainset, batch_size=512, shuffle=True)\n",
    "\n",
    "model = KAN([12, 64, 32, 1])\n",
    "model.to(torch.float32).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4, foreach=False)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
    "        for i, (X, y) in enumerate(pbar):\n",
    "            X, y  = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "    train_losses.append(loss.item())\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X).cpu().numpy()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(test_y.cpu(), y_pred)\n",
    "r2 = r2_score(test_y.cpu(), y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet model\n",
    "import torchvision.models as models\n",
    "\n",
    "X = regression_data.drop(['const'], axis=1)\n",
    "y = data['Price ($)']\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_X = torch.tensor(train_X.values, dtype=torch.float32).to(device)\n",
    "train_y = torch.tensor(train_y.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "test_X = torch.tensor(test_X.values, dtype=torch.float32).to(device)\n",
    "test_y = torch.tensor(test_y.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "trainset = TensorDataset(train_X, train_y)\n",
    "trainloader = DataLoader(trainset, batch_size=512, shuffle=True)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.resnet = models.resnet101()\n",
    "\n",
    "        self.resnet.conv1 = nn.Conv2d(self.num_classes, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(2048, 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_classes, 1, 1)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "model = ResNet(12)\n",
    "model.to(device)\n",
    "model.float()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4, foreach=False)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
    "        for i, (X, y) in enumerate(pbar):\n",
    "            X, y  = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "    train_losses.append(loss.item())\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X).cpu().numpy()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(test_y.cpu(), y_pred)\n",
    "r2 = r2_score(test_y.cpu(), y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
